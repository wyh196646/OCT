{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "from pathlib import *\n",
    "import multiprocessing\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import *\n",
    "from itertools import *\n",
    "from functools import *\n",
    "from sklearn.metrics import *\n",
    "from scipy.stats import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "from datastatics import *\n",
    "from oct_Utils import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "map_matrix='''[ [0  0  0  0  0  0  0  0  0  0]\n",
    "                [0  0  0  46 44 43 42 0  0  0]\n",
    "                [0  0  46 45 44 43 41 43 0  0]\n",
    "                [0  45 46 45 45 45 42 42 41 0]\n",
    "                [46 46 47 46 50 50 50 3  41 0]\n",
    "                [9  9  9  6  8  6  7  3  16 0]\n",
    "                [0  10 8  7  6  8  9  13 17 0]\n",
    "                [0  0  8  8  10 10 12 15 0  0]\n",
    "                [0  0  0  9  10 11 14 0  0  0]\n",
    "                [0  0  0  0  0  0  0  0  0  0]]'''#为了凑54个点，给两个视野盲区的点都随便指向了一个2\n",
    "\n",
    "\n",
    "def str_to_np_array(row, dtype=np.float):\n",
    "    row = row.replace('[', '').replace(']', '')\n",
    "    row = row.split(' ')\n",
    "    row = filter(lambda x: x != '', row)\n",
    "    row = map(lambda x: dtype(x), row)\n",
    "    row = np.array(list(row))\n",
    "    return row\n",
    "\n",
    "def str_to_np_mat(s, dtype=np.float):\n",
    "    s = s[1:-1]\n",
    "    rows = s.split('\\n')\n",
    "    for index, row in enumerate(rows):\n",
    "        rows[index] = str_to_np_array(row, dtype)\n",
    "    rows = np.stack(rows, axis=0)\n",
    "    return rows\n",
    "\n",
    "import itertools\n",
    "def calculate_position(arr):\n",
    "\t#拼接数组函数\n",
    "    array_num = list(itertools.chain.from_iterable(arr))\n",
    "    array_num = np.array(array_num)\n",
    "    key = np.unique(array_num)\n",
    "    result = {}\n",
    "    for k in key:\n",
    "        index=np.argwhere(arr == k)#返回所有符合条件的索引值，更多的方法和类参数需要看\n",
    "        v=[tuple(index[i]) for i in range(0,len(index))]\n",
    "        result[k] = v\n",
    "    result = {key:val for key, val in result.items() if  len(val)<9}\n",
    "\n",
    "    valid_point=[]\n",
    "    for key,value in result.items():\n",
    "        valid_point+=value\n",
    "\n",
    "    return result,valid_point\n",
    "\n",
    "num_mat=str_to_np_mat(map_matrix)\n",
    "countNums,valid_point=calculate_position(num_mat)#之前statics的时候，统计的数据是用的repeat的重复数据，这次要重新处理，尽量保证模块化，工程化"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "res=reverse_dict(countNums)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "data=pd.read_csv('data.csv')#\n",
    "data['np_mat_num']=data['num'].apply(str_to_np_mat)\n",
    "data['np_mat_num']=data['np_mat_num'].mask(data['eye']=='OD',data['np_mat_num'].apply(lambda x:np.flip(x,axis=1)))#all_num已经翻转过，所以数据是正常的,这里相当于\n",
    "#把label直接翻转好了，后续根据input直接做就可以了\n",
    "all_num=data['np_mat_num']\n",
    "\n",
    "def split_np_matrix(all_num,data:pd.DataFrame,valid_point):\n",
    "    res=np.zeros((1,100))\n",
    "    for num in all_num:\n",
    "        res=np.concatenate((res,np.expand_dims(num.flatten(),0)),axis=0)\n",
    "    res=res[1:]\n",
    "    for i in range(0,res.shape[1]):\n",
    "        if (i//10,i%10) in valid_point:\n",
    "            data[f'({i//10},{(i)%10})']=res[:,i]\n",
    "    return data\n",
    "\n",
    "    \n",
    "new_data=split_np_matrix(all_num,data,valid_point)\n",
    "#new_data.to_csv('new_data.csv')#这里用excel preview看数据就出现了19.0，但实际上通过调用columns属性就可以得出其实数据属性列是正常的\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "df=pd.read()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "id_base = 'pid'\n",
    "date = '1004'\n",
    "label_type = 'num'\n",
    "\n",
    "for model, size, pos in product(['r50'], ['380'], ['disc', 'macula']):\n",
    "    exp_name = f'{date}/{pos}_{label_type}_{model}_{size}'\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp = df_tmp[df_tmp['pos'] == pos]\n",
    "    df_tmp['dataset'] = df_tmp[id_base].map(to_dataset_mapping(df_tmp[id_base].tolist(), 5))\n",
    "    ensure_path(f'output/{exp_name}/exp-{id_base}/tasks')\n",
    "    df_tmp.to_csv(f'output/{exp_name}/exp-{id_base}/tasks/data.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8a1fe65d3a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'r50'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'380'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'disc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'macula'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mexp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{date}/{pos}_{label_type}_{model}_{size}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_base\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_dataset_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_base\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "2092ff7edfec3b56fec5652af09570141a5419527a294e25af3dbef5a0b75afd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}